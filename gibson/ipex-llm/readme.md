# ipex-llm

the base container sets up a ready-to-go environment for leveraging our Arc a770 as an LLM compute device, both with llama.cpp and ollama.

we in particular use ollama, so the API can be hit by `openwebui`

# running

docker compose up -d